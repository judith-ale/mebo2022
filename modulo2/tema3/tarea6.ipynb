{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 5. Modelo de Mezclas Gaussianas.\n",
    "\n",
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1f/Gauss-sum.svg/800px-Gauss-sum.svg.png\" width=\"500px\" height=\"300px\" />\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. \n",
    "\n",
    "Programar el modelo de mezclas Gaussianas en una función."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Inicialización aleatoria de los parámetros: \n",
    "\n",
    "   - $\\pi_c^0$ se puede inicializar con una muestra de la distribución de Dirichlet.\n",
    "   - $\\mu_c^0$ se puede inicializar eligiendo aleatoriamente datos de la muestra para entrenar.\n",
    "   - $\\Sigma_c^0$ se puede inicializar con la matriz identidad, o como $R^T R$ donde $R$ es una matriz de números."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(X, k):\n",
    "    \"\"\"\n",
    "    Centroids initialization for K-Means algorithm.\n",
    "    :param X: Data.\n",
    "    :param int k: Number of clusters.\n",
    "    :return: Randomly chosen parameters.\n",
    "    \"\"\"\n",
    "    # Dimension\n",
    "    N, d = X.shape\n",
    "    \n",
    "    # Choose k random positive numbers between 0 and 1 that sum 1 (Hint: Use np.random.dirichlet(alpha=(1,) * k))\n",
    "    weights = ...\n",
    "    # Choose k random points from X (Hint: Use np.random.randint(N, size=k)\n",
    "    means = ...\n",
    "    # Choose k random positive definite matrices \n",
    "    # (Hint: Use np.random.rand(d, d) to generate a random matrix of d x d)\n",
    "    covariances = np.zeros((k, d, d))\n",
    "    for c in range(k):\n",
    "        R = ...\n",
    "        covariances[c, :, :] = ...\n",
    "    \n",
    "    # The weights must be an array of size k\n",
    "    assert weights.shape == (k,)\n",
    "    # The means must be a (k, d) array\n",
    "    assert means.shape == (k, d)\n",
    "    # The weights must be an (k, d, d) array\n",
    "    assert covariances.shape == (k, d, d)\n",
    "    \n",
    "    return weights, means, covariances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. E-step\n",
    "\n",
    "En este paso, debemos calcular la distribución variacional que maximiza la cota variacional con:\n",
    "\n",
    "$$\n",
    "q(t_i=c) = p(t_i=c|x_i, \\theta) = \\frac{\\mathcal{N}(x_i | \\mu_c, \\Sigma_c) \\pi_c}{\\sum_{c=1}^{k} \\pi_c \\mathcal{N}(x | \\mu_c, \\Sigma_c)} = \\frac{\\mathcal{N}(x_i | \\mu_c, \\Sigma_c) \\pi_c}{Z},\n",
    "$$\n",
    "\n",
    "donde el denominador $Z$ es simplemente un número que normaliza el numerador para que se satisfaga que $\\sum_{c=1}^{k} q(t_i=c) = 1$.\n",
    "\n",
    "En este sentido una manera de implementar lo anterior es:\n",
    "\n",
    "1. Calcular:\n",
    "\n",
    "    $$\n",
    "    \\tilde{q}(t_i=c) = \\pi_c \\mathcal{N}(x_i | \\mu_c, \\Sigma_c).\n",
    "    $$\n",
    "\n",
    "2. Normalizar:\n",
    "\n",
    "    $$\n",
    "    q(t_i = c) = \\frac{\\tilde{q}(t_i=c)}{\\sum_{c=1}^{k} \\tilde{q}(t_i=c)}.\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step_gmm(X, weights, means, covariances):\n",
    "    \"\"\"\n",
    "    Performs E-step on GMM model.\n",
    "    :param X: Data.\n",
    "    :param weights: mixture component weights pi\n",
    "    :param means: mixture component means mu\n",
    "    :param covariances: mixture component covariance matrices sigma\n",
    "    :return: Variational distribution q.\n",
    "    \"\"\"\n",
    "    # Dimensions\n",
    "    N, d = X.shape\n",
    "    k = weights.shape[0]\n",
    "    \n",
    "    # Pre-alloc of variational distribution q(t)\n",
    "    q_tilde = np.zeros((N, k))\n",
    "\n",
    "    # Calculate unnormalized distribution \n",
    "    # (Hint: Use multivariate_normal.pdf(X, mean=..., cov=...) for the likelihood N(x | mu, sigma))\n",
    "    for c in range(k):\n",
    "        likelihood = ...\n",
    "        q_tilde[:, c] = ...\n",
    "    \n",
    "    # Normalize\n",
    "    q = q_tilde / q_tilde.sum(axis=1).reshape(-1, 1)\n",
    "    \n",
    "    # The variational distribution q must be a (k, d) array\n",
    "    assert q.shape == (N, k)\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. M-step\n",
    "\n",
    "En este paso calculamos los parámetros para maximizar \n",
    "\n",
    "$$\n",
    "\\max_{\\theta} \\sum_{i=1}^{N} \\mathbb{E}_{q(t_i)} \\left[\\log p(x_i, t_i| \\theta)\\right] = \\sum_{i=1}^{N} \\sum_{c=1}^{k} q(t_i=c) \\log \\left[\\mathcal{N}(x_i | \\mu_c, \\Sigma_c) \\pi_c\\right].\n",
    "$$\n",
    "\n",
    "Esto resulta en:\n",
    "\n",
    "$$\n",
    "\\mu_c = \\frac{\\sum_{i=1}^{N} q(t_i=c)x_i}{\\sum_{i=1}^{N} q(t_i=c)},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma_c = \\frac{\\sum_{i=1}^{N} q(t_i=c)(x_i - \\mu_c) (x_i - \\mu_c)^T}{\\sum_{i=1}^{N} q(t_i=c)},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\pi_c = \\frac{\\sum_{i=1}^{N} q(t_i=c)}{N}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_step_gmm(X, q):\n",
    "    \"\"\"\n",
    "    Performs M-step on GMM model.\n",
    "    :param X: Data.\n",
    "    :param q: Variational distribution q.\n",
    "    :return: Next iteration of parameters weights, means, covariances.\n",
    "    \"\"\"\n",
    "    # Dimensions\n",
    "    N, d = X.shape\n",
    "    k = q.shape[1]\n",
    "\n",
    "    # Means\n",
    "    means = ...\n",
    "    # Covariances\n",
    "    covariances = np.zeros((k, d, d))\n",
    "    for c in range(k):\n",
    "        covariances[c, :, :] = ...\n",
    "    # Weights\n",
    "    weights = ...\n",
    "    \n",
    "    # The weights must be an array of size k\n",
    "    assert weights.shape == (k,)\n",
    "    # The means must be a (k, d) array\n",
    "    assert means.shape == (k, d)\n",
    "    # The weights must be an (k, d, d) array\n",
    "    assert covariances.shape == (k, d, d)\n",
    "    \n",
    "    return weights, means, covariances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calcular log-verosimilitud\n",
    "\n",
    "Finalmente, necesitamos una función para dar seguimiento a la convergencia del algoritmo. Pueden haber varias selecciones para esta función:\n",
    "\n",
    "1. La cota variacional.\n",
    "2. La log-verosimilitud.\n",
    "\n",
    "Lo que haremos es que pararemos la iteraciones cuando alguna de estas funciones no varíe significativamente. \n",
    "\n",
    "Otra utilidad de esta función es que nos sirve para verificar que el algoritmo esté funcionando correctamente. Debemos recordar que este algoritmo nos asegura que esta función no puede decrecer. Si lo hace es porque algo estamos haciendo mal.\n",
    "\n",
    "Programemos la log-verosimilitud (promediada por el número de puntos):\n",
    "\n",
    "$$\n",
    "g(\\theta) := \\frac{1}{N}\\log p(X | \\theta) = \\frac{1}{N}\\sum_{i=1}^{N} \\log \\left( \\sum_{c=1}^{k} \\pi_c \\mathcal{N}(x_i | \\mu_c, \\Sigma_c) \\right).\n",
    "$$\n",
    "\n",
    "**Esta ya la hicimos en clase, no hay necesidad de que la vuelvan a hacer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_gmm(X, weights, means, covariances):\n",
    "    \"\"\"\n",
    "    Log-likelihood of the data wrt Gaussian Mixture Model.\n",
    "    :param data: Data.\n",
    "    :param mu: Means of the components of the GMM.\n",
    "    :param sigma: Covariances of the components of the GMM.\n",
    "    :param pi: Weights of the components of the GMM.\n",
    "    :return: Log-likelihood of the data wrt GMM.\n",
    "    \"\"\"\n",
    "    # Number of components\n",
    "    k = means.shape[0]\n",
    "    # Number of points\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    # Individual likelihood of each point to each normal\n",
    "    ind_likelihood = np.zeros((N, k))\n",
    "    for c in range(k):\n",
    "        ind_likelihood[:, c] = multivariate_normal.pdf(X, mean=mumeans[c, :], cov=covariances[c, :, :])\n",
    "    \n",
    "    # Log likelihood\n",
    "    log_likelihood = np.log(ind_likelihood.dot(weights)).mean()\n",
    "        \n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Juntemos todo\n",
    "\n",
    "Ya tenemos todos los componentes para nuestro modelo de mezclas Gaussianas. Haremos lo siguiente:\n",
    "\n",
    "1. Inicializaremos los parámetros $\\pi$, $\\mu$, y $\\Sigma$.\n",
    "2. Iteraremos hasta que:\n",
    "   - la log-verosimilitud no cambie significativamente, es decir hasta que: $\\left\\lvert \\frac{g(\\theta^{j}) - g(\\theta^{j-1})}{g(\\theta^{j-1})}\\right\\rvert \\leq \\epsilon$, donde $\\epsilon$ es una tolerancia para la convergencia (usualmente $\\epsilon = 10^{-3}$ o menor).\n",
    "   - Alternativamente, iteraremos un máximo número de veces. Esto para prevenir que el algoritmo se quede iterando por mucho tiempo.\n",
    "3. Este proceso lo repetiremos varias veces, esto debido a que el algoritmo de maximización de la esperanza solo alcanza máximos locales, entonces intentaremos con varias inicializaciones y nos quedaremos con la mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import inf\n",
    "def gmm(X, k, eps=1e-3, max_iter=100, restarts=50):\n",
    "    \"\"\"\n",
    "    Starts with random initialization *restarts* times. Runs optimization until saturation with *eps* reached\n",
    "    or *max_iter* iterations were made.\n",
    "    :param X: Data.\n",
    "    :param k: Number of components.\n",
    "    :param eps: Tolerance for convergence.\n",
    "    :param max_iter: Maximum number of iterations at each run.\n",
    "    :param restarts: Number of restarts.\n",
    "    :return: Array of\n",
    "    \"\"\"\n",
    "    # Dimensions\n",
    "    N, d = X.shape # number of objects\n",
    "    \n",
    "    best_loss = -inf\n",
    "    best_weights = None\n",
    "    best_means = None\n",
    "    best_covariances = None\n",
    "\n",
    "    for _ in range(restarts):\n",
    "        try:\n",
    "            # Parameters initialization\n",
    "            weights, means, covariances = init_params(X, k)\n",
    "            loss = [-inf]\n",
    "            for _ in range(max_iter):\n",
    "                try:\n",
    "                    # E-step\n",
    "                    q = e_step_gmm(X, weights, means, covariances)\n",
    "                    # M-step\n",
    "                    weights, means, covariances = m_step_gmm(X, q)\n",
    "                    # Log-likelihood\n",
    "                    loss.append(log_likelihood_gmm(X, weights, means, covariances))\n",
    "                    # Check for convergence\n",
    "                    if np.abs((loss[-1] - loss[-2]) / loss[-2]) <= eps:\n",
    "                        # Keep the best params\n",
    "                        if loss[-1] > best_loss:\n",
    "                            best_loss = loss[-1]\n",
    "                            best_weights = weights\n",
    "                            best_means = means\n",
    "                            best_covariances = covariances\n",
    "                        break\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"Singular matrix: components collapsed\")\n",
    "            pass\n",
    "\n",
    "    return best_loss, best_weights, best_means, best_covariances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, probemos nuestro algoritmo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos función para generar datos\n",
    "from bank_customer_data import generate_bank_customer_data\n",
    "# Importamos pyplot\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos datos\n",
    "data = generate_bank_customer_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss, best_weights, best_means, best_covariances = gmm(data[[\"income\", \"debt\"]].to_numpy(), k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussianas ajustadas\n",
    "X1 = multivariate_normal(mean=best_means[0, :], cov=best_covariances[0, :, :])\n",
    "X2 = multivariate_normal(mean=best_means[1, :], cov=best_covariances[1, :, :])\n",
    "X3 = multivariate_normal(mean=best_means[2, :], cov=best_covariances[2, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos\n",
    "plt.scatter(data[\"income\"], data[\"debt\"], c=\"gray\", alpha=0.6)\n",
    "\n",
    "# Gaussiana 1\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = np.linspace(0, 8, 100)\n",
    "x, y = np.meshgrid(x, y)\n",
    "z = X1.pdf(np.dstack([x, y]))\n",
    "plt.contour(x, y, z)\n",
    "\n",
    "# Gaussiana 2\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = np.linspace(0, 8, 100)\n",
    "x, y = np.meshgrid(x, y)\n",
    "z = X2.pdf(np.dstack([x, y]))\n",
    "plt.contour(x, y, z)\n",
    "\n",
    "# Gaussiana 3\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = np.linspace(0, 8, 100)\n",
    "x, y = np.meshgrid(x, y)\n",
    "z = X3.pdf(np.dstack([x, y]))\n",
    "plt.contour(x, y, z)\n",
    "\n",
    "plt.xlabel(\"Ingresos mensuales (x100k MXN)\")\n",
    "plt.ylabel(\"Deuda (x100k MXN)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por colores\n",
    "plt.scatter(data[\"income\"], data[\"debt\"], c=gmm.predict_proba(X), alpha=0.6)\n",
    "plt.xlabel(\"Ingresos mensuales (x100k MXN)\")\n",
    "plt.ylabel(\"Deuda (x100k MXN)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#808080; background:#fff;\">\n",
    "Created with Jupyter by Esteban Jiménez Rodríguez.\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
